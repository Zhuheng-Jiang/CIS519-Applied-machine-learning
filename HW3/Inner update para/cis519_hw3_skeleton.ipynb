{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cis519_hw3_skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIO3UIZe6wsZ",
        "colab_type": "text"
      },
      "source": [
        "# CIS 419/519 \n",
        "#**Homework 3 : Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gS022EH9_-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjPfIJ5G52It",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UodjntNc6Ex2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 100000, initTheta = None):       # maxIterNum = 10000   epsilon = 0.001 alpha = 0.01\n",
        "        '''\n",
        "        Constructor\n",
        "        Arguments:\n",
        "        \talpha is the learning rate\n",
        "        \tregLambda is the regularization parameter\n",
        "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
        "        \tepsilon is the convergence parameter\n",
        "        \tmaxNumIters is the maximum number of iterations to run\n",
        "          initTheta is the initial theta value. This is an optional argument\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.regLambda = regLambda\n",
        "        self.regNorm = regNorm\n",
        "        self.epsilon = epsilon\n",
        "        self.maxNumIters = maxNumIters\n",
        "        self.theta = initTheta  # give the initial theta to the theta\n",
        "        self.X_train_mean = None  # mean value of standardization\n",
        "        self.X_train_std = None   # standard deviation\n",
        "\n",
        "    \n",
        "\n",
        "    def computeCost(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-by-1 numpy matrix\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
        "        '''\n",
        "        z = np.dot(X, theta)\n",
        "        yhat = self.sigmoid(z)  # n-by-1 vector h_theta\n",
        "        # the theta zero is not regularized, using L2 norm\n",
        "        if self.regNorm == 1:   # 1st Norm for regularization\n",
        "            Lreg = -1 * (np.dot(y.T, np.log(yhat)) + np.dot((1 - y).T, np.log(1-yhat))) + regLambda * (np.linalg.norm(theta[1:], ord=1))\n",
        "        elif self.regNorm == 2: # 2nd Norm for regularization\n",
        "            Lreg = -1 * (np.dot(y.T, np.log(yhat)) + np.dot((1 - y).T, np.log(1-yhat))) + regLambda * ((np.linalg.norm(theta[1:]))**2)\n",
        "        else:\n",
        "            print(\"regNorm is not defined\")\n",
        "        cost_value = Lreg.tolist()[0][0]    # convert the matrix to scalar\n",
        "        return cost_value\n",
        "    \n",
        "    \n",
        "    def computeGradient(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the gradient of the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-by-1 numpy matrix\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            the gradient, an d-dimensional vector\n",
        "        '''\n",
        "        n, d = X.shape      # theta is d shape\n",
        "        z = np.dot(X, theta)\n",
        "        yhat = self.sigmoid(z)  # n-by-1 vector h_theta\n",
        "        gradient = np.zeros((d, 1))\n",
        "        # extract the alpha outside of the gradient, and use different formula for gradient\n",
        "        if self.regNorm == 1:  # L1\n",
        "          gradient = np.dot(np.transpose(X), (yhat-y))      # the first row of X is 1\n",
        "          gradient[1:] = gradient[1:] + regLambda * theta[1:]/np.absolute(theta[1:]) # regularization term\n",
        "        elif self.regNorm == 2: #L2 \n",
        "          gradient = np.dot(np.transpose(X), (yhat-y))      # the first row of X is 1\n",
        "          gradient[1:] = gradient[1:] + regLambda * theta[1:] # regularization term\n",
        "        return gradient    # include the negative into the gradient\n",
        "    \n",
        "    def hasConverged(self, theta_new, theta_old):\n",
        "        '''\n",
        "        Check if the gradient has converged/ L2 norm less than self.epsilon\n",
        "        :param theta_new: new theta\n",
        "        :param theta_old: old theta\n",
        "        :return: True or False\n",
        "        '''\n",
        "        if np.linalg.norm(theta_old-theta_new) <= self.epsilon:\n",
        "            # print('Has converged!')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def gradientDescent(self, X, y, theta):     # the X should be preprocessed\n",
        "        '''\n",
        "        This function is for implementing the gradient descent to update the theta\n",
        "        X: proprocessed n by d\n",
        "        y: labels\n",
        "        theta\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        for iter in range(self.maxNumIters):\n",
        "            theta_old = theta.copy()\n",
        "            theta = theta - self.alpha*self.computeGradient(theta, X, y, self.regLambda)     # gradient is negative, +\n",
        "            # print('Cost is: '+str(self.computeCost(theta, X, y, self.regLambda)))    # indicator of Cost value\n",
        "            if iter > 0 and self.hasConverged(theta, theta_old) is True:\n",
        "                print('logReg convergencd Iteration: ' + str(iter))\n",
        "                print('Cost is: '+str(self.computeCost(theta, X, y, self.regLambda))) \n",
        "                break   # the gradient descent has converged\n",
        "        return theta\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "            y is an n-by-1 Pandas data frame\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before fit() is called.\n",
        "        '''\n",
        "        # process the X set and standardization\n",
        "        X_copy = X.copy().to_numpy()    # in case the new operation affects the original X, and convert df to np\n",
        "        y_copy = y.copy().to_numpy()\n",
        "\n",
        "        n = len(y)\n",
        "        # self.X_train_mean = X_copy.mean(0)\n",
        "        # self.X_train_std = X_copy.std(0)\n",
        "        # X_scaled = (X_copy - X_copy.mean(0))/X_copy.std(0)\n",
        "        X_fit = np.c_[np.ones((n, 1)), X_copy]  # add the first column\n",
        "\n",
        "        n, d = X_fit.shape          # now the X has been added the first column\n",
        "        y_fit = y_copy.reshape(n, 1)\n",
        "\n",
        "        if self.theta is None:  # initialize the theta\n",
        "            self.theta = np.matrix(np.random.rand(d, 1) - 0.5)    # why initial with random integer rather than zeros????\n",
        "\n",
        "        # theta_copy = self.theta.copy()    # copy the theta (not sure if necessary)\n",
        "        self.theta = self.gradientDescent(X_fit, y_fit, self.theta)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "        Returns:\n",
        "            an n-by-1 dimensional Pandas data frame of the predictions\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before predict() is called.\n",
        "        '''\n",
        "        y_raw = self.predict_proba(X)\n",
        "        y_pre = y_raw.iloc[:, 0].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "        return y_pre\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        '''\n",
        "        Used the model to predict the class probability for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "        Returns:\n",
        "            an n-by-1 Pandas data frame of the class probabilities\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before predict_proba() is called.\n",
        "        '''\n",
        "        X_copy = X.copy().to_numpy()\n",
        "        n, d = X_copy.shape\n",
        "        X_scaled = np.c_[np.ones((n, 1)), X_copy]\n",
        "        return pd.DataFrame(self.sigmoid(X_scaled*self.theta))\n",
        "\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "      sigmoid = 1 / (1 + np.exp(-Z))\n",
        "      return sigmoid\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-_IFEK6g4Q",
        "colab_type": "text"
      },
      "source": [
        "# Test Logistic Regression 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Un3fMJ6keB",
        "colab_type": "code",
        "outputId": "28ad54a6-57f4-42e4-89d0-123fba71798a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "# Test script for training a logistic regressiom model\n",
        "#\n",
        "# This code should run successfully without changes if your implementation is correct\n",
        "#\n",
        "from numpy import loadtxt, ones, zeros, where\n",
        "import numpy as np\n",
        "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def test_logreg1():\n",
        "    # load the data\n",
        "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data1.csv\"\n",
        "\n",
        "    df = pd.read_csv(filepath, header=None)\n",
        "\n",
        "    X = df[df.columns[0:2]]\n",
        "    y = df[df.columns[2]]\n",
        "    # print(X,y)\n",
        "    n,d = X.shape\n",
        "    # print('X shape:' + str(n)+str(d))\n",
        "\n",
        "    # # Standardize features\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    standardizer = StandardScaler()\n",
        "    Xstandardized = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
        "    \n",
        "    # train logistic regression\n",
        "    regLambda_input = 1E-8\n",
        "    regNorm_input=2\n",
        "    logregModel = LogisticRegression(regLambda = regLambda_input, regNorm=regNorm_input)      # 1e-8 1e-2 1 2\n",
        "    logregModel.fit(Xstandardized,y)\n",
        "    \n",
        "    # Plot the decision boundary\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min = X[X.columns[0]].min() - .5\n",
        "    x_max = X[X.columns[0]].max() + .5\n",
        "    y_min = X[X.columns[1]].min() - .5\n",
        "    y_max = X[X.columns[1]].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
        "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
        "    Z = logregModel.predict(allPoints)\n",
        "    Z = np.asmatrix(Z.to_numpy())\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(1, figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    # Plot the training points\n",
        "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
        "    \n",
        "    # Configure the plot display\n",
        "    plt.xlabel('Exam 1 Score')\n",
        "    plt.ylabel('Exam 2 Score')\n",
        "    plt.title('regLambda = '+ str(regLambda_input)+'   regNorm: L'+str(regNorm_input))\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    \n",
        "    plt.show()\n",
        "test_logreg1()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "logReg convergencd Iteration: 803\n",
            "Cost is: 20.34985497519247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFyCAYAAABSqxssAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9HL_cUX6NYm",
        "colab_type": "text"
      },
      "source": [
        "# Map Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7uldP0A6Hcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapFeature(X, column1, column2, maxPower = 6):\n",
        "    '''\n",
        "    Maps the two specified input features to quadratic features. Does not standardize any features.\n",
        "        \n",
        "    Returns a new feature array with d features, comprising of\n",
        "        X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, ... up to the maxPower polynomial\n",
        "        \n",
        "    Arguments:\n",
        "        X is an n-by-d Pandas data frame, where d > 2\n",
        "        column1 is the string specifying the column name corresponding to feature X1\n",
        "        column2 is the string specifying the column name corresponding to feature X2\n",
        "    Returns:\n",
        "        an n-by-d2 Pandas data frame, where each row represents the original features augmented with the new features of the corresponding instance\n",
        "        the first bias row is not added here\n",
        "    '''\n",
        "    X_1 = X[column1]   # the first column of the X, X1\n",
        "    X_2 = X[column2]   # the second column of X, X2\n",
        "    X_map = pd.concat([X_1,X_2], axis=1)    # define the new X\n",
        "    for d in range(1, maxPower):\n",
        "      for i in range(d+2): # 0, 1, 2, ..., d+1\n",
        "        x_2_new = X_2.pow(i)    # axis \n",
        "        x_1_new = X_1.pow(d+1-i)\n",
        "        new_col = x_1_new.multiply(x_2_new)\n",
        "        X_map = pd.concat([X_map, new_col], axis=1)\n",
        "    print(X_map)\n",
        "    return X_map\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcBEI53O6lde",
        "colab_type": "text"
      },
      "source": [
        "# Test Logistic Regression 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGqdTWMU6oDH",
        "colab_type": "code",
        "outputId": "cd12ca80-d7b7-4d00-f582-c4eff1ae1bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "from numpy import loadtxt, ones, zeros, where\n",
        "import numpy as np\n",
        "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "def test_logreg2():\n",
        "\n",
        "    polyPower = 6\n",
        "\n",
        "    # load the data\n",
        "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data2.csv\"\n",
        "    df = pd.read_csv(filepath, header=None)\n",
        "\n",
        "    X = df[df.columns[0:2]]\n",
        "    y = df[df.columns[2]]\n",
        "\n",
        "    n,d = X.shape\n",
        "\n",
        "    # map features into a higher dimensional feature space\n",
        "    Xaug = mapFeature(X.copy(), X.columns[0], X.columns[1], polyPower)\n",
        "\n",
        "    # # Standardize features\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    standardizer = StandardScaler()\n",
        "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # compute mean and stdev on training set for standardization\n",
        "    \n",
        "    # train logistic regression\n",
        "    logregModel = LogisticRegressionAdagrad(regLambda = 0.00000001, regNorm=2)      # this line is changed for testing 1e-8 1e-2 1 2\n",
        "    logregModel.fit(Xaug,y)\n",
        "    \n",
        "    # Plot the decision boundary\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min = X[X.columns[0]].min() - .5\n",
        "    x_max = X[X.columns[0]].max() + .5\n",
        "    y_min = X[X.columns[1]].min() - .5\n",
        "    y_max = X[X.columns[1]].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
        "    allPoints = mapFeature(allPoints, allPoints.columns[0], allPoints.columns[1], polyPower)\n",
        "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
        "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # standardize data\n",
        "    \n",
        "    Z = logregModel.predict(allPoints)\n",
        "    Z = np.asmatrix(Z.to_numpy())\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure(1, figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    # Plot the training points\n",
        "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
        "    \n",
        "    # Configure the plot display\n",
        "    plt.xlabel('Microchip Test 1')\n",
        "    plt.ylabel('Microchip Test 2')\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(str(Z.min()) + \" \" + str(Z.max()))\n",
        "\n",
        "test_logreg2()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ce7139e30574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtest_logreg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ce7139e30574>\u001b[0m in \u001b[0;36mtest_logreg2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# map features into a higher dimensional feature space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mXaug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyPower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# # Standardize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mapFeature' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ef6eUW7BNy",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression with Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zcisRww7Y3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegressionAdagrad:\n",
        "\n",
        "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10000, initTheta = None):     # e = 1e-4\n",
        "        '''\n",
        "        Constructor\n",
        "        Arguments:\n",
        "        \talpha is the learning rate\n",
        "        \tregLambda is the regularization parameter\n",
        "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
        "        \tepsilon is the convergence parameter\n",
        "        \tmaxNumIters is the maximum number of iterations to run\n",
        "          initTheta is the initial theta value. This is an optional argument\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.regLambda = regLambda\n",
        "        self.regNorm = regNorm\n",
        "        self.epsilon = epsilon\n",
        "        self.maxNumIters = maxNumIters\n",
        "        self.theta = initTheta  # give the initial theta to the theta\n",
        "        self.X_train_mean = None  # mean value of standardization\n",
        "        self.X_train_std = None   # standard deviation\n",
        "    \n",
        "\n",
        "    def computeCost(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-by-1 numpy matrix\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
        "        '''\n",
        "        z = np.dot(X, theta)\n",
        "        yhat = self.sigmoid(z)  # n-by-1 vector h_theta\n",
        "        # the theta zero is not regularized, using L2 norm\n",
        "        if self.regNorm == 1:   # 1st Norm for regularization\n",
        "            Lreg = -1 * (np.dot(y.T, np.log(yhat)) + np.dot((1 - y).T, np.log(1-yhat))) + regLambda * (np.linalg.norm(theta[1:], ord=1))\n",
        "        elif self.regNorm == 2: # 2nd Norm for regularization\n",
        "            Lreg = -1 * (np.dot(y.T, np.log(yhat)) + np.dot((1 - y).T, np.log(1-yhat))) + regLambda * ((np.linalg.norm(theta[1:]))**2)\n",
        "        else:\n",
        "            print(\"regNorm is not defined\")\n",
        "        cost_value = Lreg.tolist()[0][0]    # convert the matrix to scalar\n",
        "        return cost_value\n",
        "    \n",
        "    \n",
        "    def computeGradient(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the gradient of the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-by-1 numpy matrix\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            the gradient, an d-dimensional vector\n",
        "        '''\n",
        "        X_copy = X.copy()\n",
        "        d = len(X_copy)      # theta is d shape d=1 now\n",
        "        X_copy = X_copy.reshape((1, d))   # (n,) to (n,1)\n",
        "        z = np.dot(X_copy, theta)\n",
        "        yhat = self.sigmoid(z)  # one value\n",
        "        gradient = np.zeros((d, 1))\n",
        "        # extract the alpha outside of the gradient, and use different formula for gradient\n",
        "        if self.regNorm == 1:  # L1\n",
        "          gradient = np.dot(X_copy.T, (yhat-y))      # the first row of X is 1\n",
        "          gradient[1:] = gradient[1:] + regLambda * theta[1:]/np.absolute(theta[1:]) # regularization term\n",
        "        elif self.regNorm == 2: #L2 \n",
        "          gradient = np.dot(X_copy.T, (yhat-y))      # the first row of X is 1\n",
        "          gradient[1:] = gradient[1:] + regLambda * theta[1:] # regularization term\n",
        "        return gradient    # include the negative into the gradient\n",
        "    \n",
        "    def hasConverged(self, theta_new, theta_old):\n",
        "        '''\n",
        "        Check if the gradient has converged/ L2 norm less than self.epsilon\n",
        "        :param theta_new: new theta\n",
        "        :param theta_old: old theta\n",
        "        :return: True or False\n",
        "        '''\n",
        "        if np.linalg.norm(theta_old-theta_new) <= self.epsilon:\n",
        "            # print('Has converged!')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def gradientDescent(self, X, y, theta):     # the X should be preprocessed\n",
        "        '''\n",
        "        This function is for implementing the gradient descent to update the theta\n",
        "        X: proprocessed n by d\n",
        "        y: labels\n",
        "        theta\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        X_copy = X.copy()\n",
        "        y_copy = y.copy()\n",
        "        dataset = np.concatenate((X_copy, y_copy), axis=1)   # combine the X and y for shuffling\n",
        "        G = np.zeros((d,1))\n",
        "        min_const = 1E-5\n",
        "\n",
        "        iter_break = 0            # flag of outer \n",
        "\n",
        "        for iter in range(self.maxNumIters):\n",
        "            # dataset = np.concatenate((X_copy, y_copy), axis=1)   # combine the X and y for shuffling\n",
        "            np.random.shuffle(dataset)  # shuffle the dataset for every outter iteration  # shuffle function\n",
        "            y_shuffled = dataset[:, -1]  # separate the last column as y numpy.array\n",
        "            X_shuffled = dataset[:, 0:-1]  # drop the y column\n",
        "            theta_old = theta.copy()  # store the old theta of last outter iteration\n",
        "            g = np.zeros((d, 1))    # store the gradient\n",
        "            for ins_num in range(n):  # instance n is number of instances\n",
        "                x_temp = X_shuffled[ins_num, :]\n",
        "                y_temp = y_shuffled[ins_num]\n",
        "                curr_grad = self.computeGradient(theta, x_temp, y_temp, self.regLambda)  # a vector\n",
        "                g = np.power(curr_grad, 2)    # get the g as gradient's square\n",
        "                \n",
        "                G += g                # accumulate the g to G   d,1 vector\n",
        "                curr_alpha = self.alpha / (np.sqrt(G) + min_const)    # curr_alpah is d*1 vector\n",
        "                theta = theta - np.multiply(curr_alpha, curr_grad)   # curr_grad d*1 compGrad d*1\n",
        "\n",
        "                # if iter > 0 and self.hasConverged(theta, theta_old) is True:\n",
        "                #     iter_break = 1 \n",
        "                #     break\n",
        "            \n",
        "            # if iter_break == 1:\n",
        "            #     break\n",
        "\n",
        "            # G += g\n",
        "            # curr_alpha = self.alpha / (np.linalg.norm(G, 2) + min_const)\n",
        "            # theta = theta - curr_alpha * self.computeGradient(theta, x_temp, y_temp, self.regLambda)\n",
        "\n",
        "            # print('==============\\nCurrent Iter is:' + str(iter + 1))  # print the iter number\n",
        "            # print('Cost: '+str(self.computeCost(theta, X_shuffled, y_shuffled, self.regLambda)))\n",
        "\n",
        "            if iter > 0 and self.hasConverged(theta, theta_old) is True:\n",
        "                print('Adagrad converged Iteration:'+str(iter))\n",
        "                break\n",
        "\n",
        "\n",
        "        return theta\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "            y is an n-by-1 Pandas data frame\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before fit() is called.\n",
        "        '''\n",
        "        # process the X set and standardization\n",
        "        X_copy = X.copy().to_numpy()    # in case the new operation affects the original X, and convert df to np\n",
        "        y_copy = y.copy().to_numpy()\n",
        "\n",
        "        n = len(y)\n",
        "        # self.X_train_mean = X_copy.mean(0)\n",
        "        # self.X_train_std = X_copy.std(0)\n",
        "        # X_scaled = (X_copy - X_copy.mean(0))/X_copy.std(0)\n",
        "        X_fit = np.c_[np.ones((n, 1)), X_copy]  # add the first column\n",
        "\n",
        "        n, d = X_fit.shape          # now the X has been added the first column\n",
        "        y_fit = y_copy.reshape(n, 1)\n",
        "\n",
        "        if self.theta is None:  # initialize the theta\n",
        "            self.theta = np.matrix(np.random.rand(d, 1) - 0.5)    # why initial with random integer rather than zeros????\n",
        "\n",
        "        # theta_copy = self.theta.copy()    # copy the theta (not sure if necessary)\n",
        "        self.theta = self.gradientDescent(X_fit, y_fit, self.theta)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "        Returns:\n",
        "            an n-by-1 dimensional Pandas data frame of the predictions\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before predict() is called.\n",
        "        '''\n",
        "        y_raw = self.predict_proba(X)\n",
        "        y_pre = y_raw.iloc[:, 0].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "        return y_pre\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        '''\n",
        "        Used the model to predict the class probability for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas data frame\n",
        "        Returns:\n",
        "            an n-by-1 Pandas data frame of the class probabilities\n",
        "        Note:\n",
        "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
        "            Standardization should be optionally done before predict_proba() is called.\n",
        "        '''\n",
        "        X_copy = X.copy().to_numpy()\n",
        "        # print(X_copy.shape)\n",
        "        # print('=======')\n",
        "        n, d = X_copy.shape\n",
        "        X_scaled = np.c_[np.ones((n, 1)), X_copy]\n",
        "        return pd.DataFrame(self.sigmoid(X_scaled*self.theta))\n",
        "\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        sigmoid = 1 / (1 + np.exp(-Z))\n",
        "        return sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP_-g1UQ9zL4",
        "colab_type": "text"
      },
      "source": [
        "2.2 Data Analysis Test Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxzpT5SS8CEZ",
        "colab_type": "code",
        "outputId": "07a0ba76-ca1f-4d57-cdc2-1fea443d10cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import loadtxt, ones, zeros, where\n",
        "import numpy as np\n",
        "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "def test_dataanalysis():\n",
        "\n",
        "    # load the data\n",
        "    # filepath = \"/content/gdrive/My Drive/Colab Notebooks/datasets/cis519_hw3/hw3-diabetes.csv\"\n",
        "    # filepath = \"/content/gdrive/My Drive/Colab Notebooks/datasets/cis519_hw3/hw3-wdbc.csv\"\n",
        "    filepath = \"/content/gdrive/My Drive/Colab Notebooks/datasets/cis519_hw3/hw3-retinopathy.csv\"\n",
        "\n",
        "    df = pd.read_csv(filepath, header=None)\n",
        "\n",
        "    #======= preprocess ======================\n",
        "    X = df[df.columns[0:-1]]  # remove the last column\n",
        "    y = df[df.columns[-1]]    # get the label\n",
        "\n",
        "    # one-hot coding to translate X\n",
        "    # X = pd.get_dummies(X)\n",
        "    # X.drop(labels=X.columns[X.dtypes == object], axis=1,inplace=True)\n",
        "    # print(X)\n",
        "\n",
        "    # binary the y label str -> 0 1   use unique to find the items\n",
        "    y_slice = df.iloc[:, -1]    # get the series of y \n",
        "    y = pd.DataFrame((y_slice == y_slice.iloc[0]).astype('int32'))    \n",
        "    #========================================\n",
        "    n,d = X.shape\n",
        "\n",
        "    # # Standardize features\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    standardizer = StandardScaler()\n",
        "    Xstandardized = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
        "    \n",
        "    # train logistic regression\n",
        "\n",
        "    # logregModel = LogisticRegression(regLambda = 0.3, regNorm=1)      # 1e-8 1e-2 1 2\n",
        "    # logregModel = LogisticRegressionAdagrad(regLambda = 0.01, regNorm=1)      # 1e-8 1e-2 1 2\n",
        "    # logregModel.fit(Xstandardized,y)\n",
        "\n",
        "    # tuneRegLambda(Xstandardized, y, )   # tuning lambda\n",
        "\n",
        "    iterValues = [1, 2, 3, 4, 5, 7, 9, 12, 16, 20,30,40,50,100,300, 500, 750, 1000, 2000]   # set the learning curve's variable\n",
        "    iter_accuracy = []\n",
        "    for i in iterValues:\n",
        "        print('========= Max Iter: '+ str(i))\n",
        "        iter_accuracy.append(tuneRegLambda(Xstandardized, y, i))\n",
        "    print(iter_accuracy)\n",
        "\n",
        "\n",
        "test_dataanalysis()"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "========= Max Iter: 1\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6472649696735062\n",
            "============================== \n",
            "Best cvScore is: 0.6472649696735062   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 2\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6597141566653761\n",
            "============================== \n",
            "Best cvScore is: 0.6597141566653761   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 3\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.670415134210866\n",
            "============================== \n",
            "Best cvScore is: 0.670415134210866   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 4\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6767929732868756\n",
            "============================== \n",
            "Best cvScore is: 0.6767929732868756   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 5\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6802803184281843\n",
            "============================== \n",
            "Best cvScore is: 0.6802803184281843   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 7\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6759602045425215\n",
            "============================== \n",
            "Best cvScore is: 0.6759602045425215   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 9\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6794082300942057\n",
            "============================== \n",
            "Best cvScore is: 0.6794082300942057   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 12\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6866329526390502\n",
            "============================== \n",
            "Best cvScore is: 0.6866329526390502   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 16\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6811241773132017\n",
            "============================== \n",
            "Best cvScore is: 0.6811241773132017   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 20\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6869465011614402\n",
            "============================== \n",
            "Best cvScore is: 0.6869465011614402   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 30\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6892179636082075\n",
            "============================== \n",
            "Best cvScore is: 0.6892179636082075   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 40\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6866601738934055\n",
            "============================== \n",
            "Best cvScore is: 0.6866601738934055   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 50\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6889780132920378\n",
            "============================== \n",
            "Best cvScore is: 0.6889780132920378   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 100\n",
            "Adagrad converged Iteration:99\n",
            "Adagrad converged Iteration:97\n",
            "Adagrad converged Iteration:98\n",
            "Adagrad converged Iteration:98\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6985377145438121\n",
            "============================== \n",
            "Best cvScore is: 0.6985377145438121   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 300\n",
            "Adagrad converged Iteration:106\n",
            "Adagrad converged Iteration:105\n",
            "Adagrad converged Iteration:116\n",
            "Adagrad converged Iteration:80\n",
            "Adagrad converged Iteration:113\n",
            "Adagrad converged Iteration:95\n",
            "Adagrad converged Iteration:110\n",
            "Adagrad converged Iteration:110\n",
            "Adagrad converged Iteration:100\n",
            "Adagrad converged Iteration:99\n",
            "Adagrad converged Iteration:101\n",
            "Adagrad converged Iteration:119\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6924260388437218\n",
            "============================== \n",
            "Best cvScore is: 0.6924260388437218   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 500\n",
            "Adagrad converged Iteration:117\n",
            "Adagrad converged Iteration:100\n",
            "Adagrad converged Iteration:106\n",
            "Adagrad converged Iteration:94\n",
            "Adagrad converged Iteration:116\n",
            "Adagrad converged Iteration:101\n",
            "Adagrad converged Iteration:93\n",
            "Adagrad converged Iteration:110\n",
            "Adagrad converged Iteration:127\n",
            "Adagrad converged Iteration:96\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:103\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.7020180023228804\n",
            "============================== \n",
            "Best cvScore is: 0.7020180023228804   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 750\n",
            "Adagrad converged Iteration:108\n",
            "Adagrad converged Iteration:101\n",
            "Adagrad converged Iteration:113\n",
            "Adagrad converged Iteration:93\n",
            "Adagrad converged Iteration:102\n",
            "Adagrad converged Iteration:93\n",
            "Adagrad converged Iteration:111\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:108\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:107\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.7060709446380178\n",
            "============================== \n",
            "Best cvScore is: 0.7060709446380178   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 1000\n",
            "Adagrad converged Iteration:111\n",
            "Adagrad converged Iteration:98\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:97\n",
            "Adagrad converged Iteration:108\n",
            "Adagrad converged Iteration:100\n",
            "Adagrad converged Iteration:112\n",
            "Adagrad converged Iteration:96\n",
            "Adagrad converged Iteration:102\n",
            "Adagrad converged Iteration:89\n",
            "Adagrad converged Iteration:93\n",
            "Adagrad converged Iteration:110\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.7057553797264162\n",
            "============================== \n",
            "Best cvScore is: 0.7057553797264162   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 2000\n",
            "Adagrad converged Iteration:92\n",
            "Adagrad converged Iteration:117\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:98\n",
            "Adagrad converged Iteration:118\n",
            "Adagrad converged Iteration:101\n",
            "Adagrad converged Iteration:116\n",
            "Adagrad converged Iteration:100\n",
            "Adagrad converged Iteration:112\n",
            "Adagrad converged Iteration:108\n",
            "Adagrad converged Iteration:97\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.7008374064395406\n",
            "============================== \n",
            "Best cvScore is: 0.7008374064395406   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 4000\n",
            "Adagrad converged Iteration:114\n",
            "Adagrad converged Iteration:96\n",
            "Adagrad converged Iteration:94\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:124\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:91\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:117\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:104\n",
            "Adagrad converged Iteration:93\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6985326735707833\n",
            "============================== \n",
            "Best cvScore is: 0.6985326735707833   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 6000\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:105\n",
            "Adagrad converged Iteration:111\n",
            "Adagrad converged Iteration:92\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:115\n",
            "Adagrad converged Iteration:94\n",
            "Adagrad converged Iteration:105\n",
            "Adagrad converged Iteration:100\n",
            "Adagrad converged Iteration:94\n",
            "Adagrad converged Iteration:114\n",
            "Adagrad converged Iteration:103\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.6982402971351143\n",
            "============================== \n",
            "Best cvScore is: 0.6982402971351143   and Best lambda is:[[1.e-08]]\n",
            "========= Max Iter: 10000\n",
            "Adagrad converged Iteration:106\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:115\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:114\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:109\n",
            "Adagrad converged Iteration:93\n",
            "Adagrad converged Iteration:98\n",
            "Adagrad converged Iteration:103\n",
            "Adagrad converged Iteration:105\n",
            "Current Lambda:1e-08\n",
            "Accuracy:0.7019806991224674\n",
            "============================== \n",
            "Best cvScore is: 0.7019806991224674   and Best lambda is:[[1.e-08]]\n",
            "[0.6472649696735062, 0.6597141566653761, 0.670415134210866, 0.6767929732868756, 0.6802803184281843, 0.6759602045425215, 0.6794082300942057, 0.6866329526390502, 0.6811241773132017, 0.6869465011614402, 0.6892179636082075, 0.6866601738934055, 0.6889780132920378, 0.6985377145438121, 0.6924260388437218, 0.7020180023228804, 0.7060709446380178, 0.7057553797264162, 0.7008374064395406, 0.6985326735707833, 0.6982402971351143, 0.7019806991224674]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN5RSRKr_day",
        "colab_type": "text"
      },
      "source": [
        "Tuning regLambda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6eWJR4wgkKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn as sk\n",
        "\n",
        "def tuneRegLambda(X, y, iter_times):   # regLambdaValues is value list\n",
        "    '''\n",
        "    This function calls the crossvalidation and tries the series of lambda\n",
        "    num_trials and folds are pre-defined\n",
        "    '''\n",
        "    # regLambdaValues = np.array([1E-8, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 10])    # for tuning \n",
        "    regLambdaValues = np.array([0.0001])      # for learning curve\n",
        "    best_Lambda = 0 \n",
        "    num_trials = 3    # typically 3\n",
        "    num_folds = 4     # typically 4\n",
        "    maxNumIters = iter_times    # for learning curve\n",
        "\n",
        "    accuracy = []\n",
        "    for test_lambda in regLambdaValues:\n",
        "        # change the model for test\n",
        "        logreg_obj = LogisticRegressionAdagrad(alpha=0.01, regLambda = test_lambda, regNorm=2, epsilon=0.001, maxNumIters=iter_times)  # state a new class project 1e-8  / change the maxNunIter\n",
        "        curr_accuracy = crossValidation(logreg_obj, X, y, num_trials, num_folds, test_lambda) \n",
        "        accuracy.append(curr_accuracy)    #store the accuracy \n",
        "        print(\"Current Lambda:\"+str(test_lambda))\n",
        "        print('Accuracy:'+str(curr_accuracy))\n",
        "        print('============================== ')\n",
        "    accuracy = np.array(accuracy)\n",
        "    best_cvScore = np.max(accuracy) \n",
        "    k = np.argwhere(accuracy == best_cvScore)  \n",
        "    best_Lambda = regLambdaValues[k]        # [accuracy.index[best_cvScore]]\n",
        "    print('Best cvScore is: '+str(best_cvScore)+'   and Best lambda is:'+ str(best_Lambda))\n",
        "    return best_cvScore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0JcodO_ZxW",
        "colab_type": "text"
      },
      "source": [
        "Cross Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMbAxw2Lg0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "def crossValidation(logreg_obj, X, y, num_trials, num_folds, regLambda):\n",
        "    '''\n",
        "    This function is for computing the model's acuuracy\n",
        "    logreg_obj is a object of class logisticRegression(Adagrad)\n",
        "\n",
        "    Return: a float value of accuracy score\n",
        "    '''\n",
        "    total_error = 0   # accumulating error\n",
        "    for trial in range(num_trials):\n",
        "        kf = KFold(n_splits=num_folds,shuffle=True,random_state=None)  # kFold object\n",
        "        for train_index, test_index in kf.split(X):   \n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "            logreg_obj.theta = None                       # clear and intialize the theta of last trial \n",
        "            logreg_obj.fit(X_train, y_train)\n",
        "            y_predict = logreg_obj.predict(X_test)\n",
        "\n",
        "            n, d = X_test.shape\n",
        "            label_test = y_test.to_numpy()\n",
        "            label_test = label_test.reshape(n,1)\n",
        "            label_pedict = y_predict.to_numpy()\n",
        "            label_pedict = label_pedict.reshape(n,1)\n",
        "\n",
        "            total_error = total_error + np.linalg.norm((label_pedict-label_test), ord=1)/n    # error rate\n",
        "    cvScore = 1 - total_error/(num_trials*num_folds)\n",
        "    # print('cv Score: '+str(cvScore))\n",
        "    return cvScore\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
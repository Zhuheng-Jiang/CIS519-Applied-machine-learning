# -*- coding: utf-8 -*-
"""hw4_skeleton.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uq2DlPYpAazit6X7ui7SD-L3OYjxZL1U

# CIS 419/519 
#**Homework 4 : Adaboost and the Challenge**
"""

import pandas as pd
import numpy as np

"""# Adaboost-SAMME"""

import numpy as np
import random
import math
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score    # for comparing the labels and y_predict

class BoostedDT:

    def __init__(self, numBoostingIters=100, maxTreeDepth=3):
        '''
        Constructor

        Class Fields 
        clfs : List object containing individual DecisionTree classifiers, in order of creation during boosting
               finally we will have T ht models after training
        betas : List of beta values, in order of creation during boosting
        '''

        self.clfs = None  # keep the class fields, and be sure to keep them updated during boosting
        self.betas = None 
        self.numBoostingIters = numBoostingIters    # the number of iterations T
        self.maxTreeDepth = maxTreeDepth
        self.K = None       # labels include how many classes 
        self.classes = None



    def fit(self, X, y, random_state=None):
        '''
        Trains the model. 
        Be sure to initialize all individual Decision trees with the provided random_state value if provided.
        
        Arguments:
            X is an n-by-d Pandas Data Frame
            y is an n-by-1 Pandas Data Frame
            random_seed is an optional integer value
        '''
        #TODO
        # get the class number K from the label y
        #

        X = X.copy().to_numpy()
        y = y.copy().to_numpy()
        n, d = X.shape  # use the whole data set for training   
        y = y.reshape((n, 1))
        y = np.where(y == 0, -1, y)
        weights = np.ones((n, 1)) / n
        num_iter = self.numBoostingIters  # get T 
        
        if self.clfs is None:   # initialize the clfs
            self.clfs = []
        if self.betas is None:  # initialize the beta list 
            self.betas = []
        self.classes = np.unique(y)        ######
        self.K = len(self.classes)     
        
        for t in range(self.numBoostingIters):   
            curr_clf = tree.DecisionTreeClassifier(max_depth=self.maxTreeDepth, random_state=random_state) # declare the tree
            # print('&&&'+str(weights.shape))
            weights_input = weights.reshape((n,))
            curr_clf.fit(X, y, sample_weight = weights_input)    # train the model
            error = 0
            y_predict = curr_clf.predict(X).reshape((n, 1))
            y_predict = np.where(y_predict == 0, -1, y_predict)
            for i in range(n):
                if y_predict[i] != y[i]:
                    error += weights[i]
            # error = np.sum(np.multiply(diff_y, weights))      # 多分类会出现负值
            # print('error = '+str(error))
            curr_beta = 1/2 * (np.log((1 - error)/error) + np.log(self.K - 1))   # more than one class

            self.clfs.append(curr_clf)      # update the current tree into the list of clfs
            self.betas.append(curr_beta)    # update the beta list
            # update the weights

            # scalar = diff_y.copy()    #########
            # scalar[scalar == 0] = -1
            scalar = (y_predict == y).astype('int32').reshape((n, 1))
            scalar = np.where(scalar == 0, -1, scalar)
            weights = weights*np.exp( -curr_beta * scalar)

            weights = weights / np.sum(weights)    # normalize the weights

    

    def predict(self, X):
        '''
        Used the model to predict values for each instance in X
        Arguments:
            X is an n-by-d Pandas Data Frame
        Returns:
            an n-by-1 Pandas Data Frame of the predictions
        '''
        #TODO
        # print("Into prediction function!")
        T = len(self.clfs)  
        n, d = X.shape    # for n instances there should be n y-labels
        X_copy = X.copy().to_numpy()
        y_predict = np.zeros((n,1))
        # y_class_pred = np.zeros((n, self.classNum))
        for t in range(T):
            curr_model = self.clfs[t] 
            curr_beta = self.betas[t]
            y_predict_t = curr_beta * curr_model.predict(X_copy).reshape((n, 1))      # get the current y preditc vector\
            print('y_predict shape: '+str(y_predict_t.shape))
            y_predict += y_predict_t
        # print('**'+str(self.betas))
        # print('**'+str(y_predict))
        y_predict = pd.DataFrame(np.where(y_predict<=0, 0, 1))
        # y_predict = np.where(y_predict == -1, 0, y_predict)
        return y_predict

"""# Test BoostedDT"""

import numpy as np
from sklearn import datasets
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def test_boostedDT():
    # load the data set     ORIGINAL
    sklearn_dataset = datasets.load_breast_cancer()
    # convert to pandas df
    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)
    df['CLASS'] = pd.Series(sklearn_dataset.target)
    df.head()

    # =============== Start the prediction (unchanged) =========================
    # split randomly into training/testing
    train, test = train_test_split(df, test_size=0.5, random_state=42)

    # Split into X,y matrices   FOR PART ONE TESTING AND COMPARING
    X_train = train.drop(['CLASS'], axis=1)
    y_train = train['CLASS']
    X_test = test.drop(['CLASS'], axis=1)
    y_test = test['CLASS']

    # ======================= Comparing accuracy ============================
    # train the decision tree
    modelDT = DecisionTreeClassifier()
    modelDT.fit(X_train, y_train)

    # train the boosted DT
    modelBoostedDT = BoostedDT(numBoostingIters=100, maxTreeDepth=2)
    modelBoostedDT.fit(X_train, y_train)

    # train sklearn's implementation of Adaboost
    modelSKBoostedDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100)
    modelSKBoostedDT.fit(X_train, y_train)

    # output predictions on the test data
    ypred_DT = modelDT.predict(X_test)
    ypred_BoostedDT = modelBoostedDT.predict(X_test)
    ypred_SKBoostedDT = modelSKBoostedDT.predict(X_test)

    print(ypred_DT, ypred_BoostedDT, ypred_SKBoostedDT)
    # compute the training accuracy of the model
    accuracy_DT = accuracy_score(y_test, ypred_DT)
    accuracy_BoostedDT = accuracy_score(y_test, ypred_BoostedDT)
    accuracy_SKBoostedDT = accuracy_score(y_test, ypred_SKBoostedDT)

    print("Decision Tree Accuracy = " + str(accuracy_DT))
    print("My Boosted Decision Tree Accuracy = " + str(accuracy_BoostedDT))
    print("Sklearn's Boosted Decision Tree Accuracy = " + str(accuracy_SKBoostedDT))
    print()
    print("Note that due to randomization, your boostedDT might not always have the ")
    print("exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they ")
    print("should be roughly equivalent and should usually exceed the standard DT.")
    # ================================================================================================
# test_boostedDT()
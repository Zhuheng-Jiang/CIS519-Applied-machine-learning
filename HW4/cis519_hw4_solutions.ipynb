{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIO3UIZe6wsZ",
        "colab_type": "text"
      },
      "source": [
        "# CIS 419/519 \n",
        "#**Homework 4 : Adaboost and the Challenge**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gS022EH9_-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjPfIJ5G52It",
        "colab_type": "text"
      },
      "source": [
        "# Adaboost-SAMME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdiCAgcNIt_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score    # for comparing the labels and y_predict\n",
        "\n",
        "class BoostedDT:\n",
        "\n",
        "    def __init__(self, numBoostingIters=100, maxTreeDepth=3):\n",
        "        '''\n",
        "        Constructor\n",
        "\n",
        "        Class Fields \n",
        "        clfs : List object containing individual DecisionTree classifiers, in order of creation during boosting\n",
        "               finally we will have T ht models after training\n",
        "        betas : List of beta values, in order of creation during boosting\n",
        "        '''\n",
        "\n",
        "        self.clfs = None  # keep the class fields, and be sure to keep them updated during boosting\n",
        "        self.betas = None \n",
        "        self.numBoostingIters = numBoostingIters    # the number of iterations T\n",
        "        self.maxTreeDepth = maxTreeDepth\n",
        "        self.weights = None       # when the training set is read, get the number of instances n and initialize it\n",
        "        self.instanceNum = 0      # initial instance number \n",
        "        self.classNum = 2         # labels include how many classes \n",
        "        self.label_class = None\n",
        "        self.alpha_t = None\n",
        "        self.alpha_sum = 0\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y, random_state=None):\n",
        "        '''\n",
        "        Trains the model. \n",
        "        Be sure to initialize all individual Decision trees with the provided random_state value if provided.\n",
        "        \n",
        "        Arguments:\n",
        "            X is an n-by-d Pandas Data Frame\n",
        "            y is an n-by-1 Pandas Data Frame\n",
        "            random_seed is an optional integer value\n",
        "        '''\n",
        "        #TODO\n",
        "        # get the class number K from the label y\n",
        "        #\n",
        "\n",
        "        X_copy = X.copy().to_numpy()\n",
        "        y_copy = y.copy().to_numpy()\n",
        "\n",
        "        num_iter = self.numBoostingIters  # get T \n",
        "        if self.clfs is None:   # initialize the clfs\n",
        "            self.clfs = []\n",
        "        if self.betas is None:  # initialize the beta list \n",
        "            self.betas = []\n",
        "\n",
        "\n",
        "        # X_train, X_test, y_train, y_test = train_test_split(X, y)               \n",
        "        # n, d = X_train.shape     \n",
        "        n, d = X_copy.shape        # use the whole data set for training      \n",
        "        self.instanceNum = n    # n is the instances number and use it to set weights\n",
        "        weights = np.ones(n)/n\n",
        "        self.label_class  = np.unique(y_copy)    # get the class in labels \n",
        "        self.classNum = len(self.label_class) \n",
        "     \n",
        "        \n",
        "        for t in range(self.numBoostingIters):   \n",
        "            curr_clf = tree.DecisionTreeClassifier(max_depth=self.maxTreeDepth, random_state=random_state) # declare the tree\n",
        "            curr_clf.fit(X_copy, y_copy, sample_weight = weights)    # train the model\n",
        "\n",
        "            y_predict = curr_clf.predict(X_copy)    # h(x)\n",
        "\n",
        "            # compute error \n",
        "            diff_y = (y_predict != y_copy).astype('int32')\n",
        "            error = (np.multiply(diff_y, weights)).sum()      # 多分类会出现负值\n",
        "            \n",
        "            # print('error = '+str(error))\n",
        "            curr_beta = 1/2 * (np.log((1 - error)/error) + np.log(self.classNum - 1))   # more than one class \n",
        "\n",
        "            self.clfs.append(curr_clf)      # update the current tree into the list of clfs\n",
        "            self.betas.append(curr_beta)    # update the beta list\n",
        "            # update the weights\n",
        "\n",
        "            scalar = diff_y.copy()    #########\n",
        "            scalar[scalar == 0] = -1 \n",
        "      \n",
        "            weights = np.multiply(weights, np.exp( curr_beta * scalar))\n",
        "            weights = weights / np.sum(weights)    # normalize the weights\n",
        "\n",
        "    \n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is an n-by-d Pandas Data Frame\n",
        "        Returns:\n",
        "            an n-by-1 Pandas Data Frame of the predictions\n",
        "        '''\n",
        "        #TODO\n",
        "        # print(\"Into prediction function!\")\n",
        "        T = len(self.clfs)  \n",
        "        n, d = X.shape    # for n instances there should be n y-labels\n",
        "        X_copy = X.copy().to_numpy()\n",
        "        y_predict = np.zeros(n)\n",
        "        y_class_pred = np.zeros((n, self.classNum))\n",
        "        for t in range(T):\n",
        "            curr_model = self.clfs[t] \n",
        "            curr_beta = self.betas[t]\n",
        "            y_predict_t = curr_model.predict(X_copy)      # get the current y preditc vector\n",
        "\n",
        "            for ins in range(n):    # for every instance (row)\n",
        "                curr_label = y_predict_t[ins] \n",
        "                y_class_pred[ins, curr_label] += curr_beta\n",
        "        y_predict = np.argmax(y_class_pred, axis=1).reshape((n, 1))\n",
        "        return y_predict\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DAfYDnGU9l8",
        "colab_type": "text"
      },
      "source": [
        "# Test BoostedDT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxDOJf2rIr2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "83b2d157-6863-470c-a9a6-0fcee2dc777e"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def test_boostedDT():\n",
        "\n",
        "  # load the data set     ORIGINAL\n",
        "  # sklearn_dataset = datasets.load_breast_cancer()\n",
        "  # convert to pandas df\n",
        "  # df = pd.DataFrame(sklearn_dataset.data,columns=sklearn_dataset.feature_names)\n",
        "  # df['CLASS'] = pd.Series(sklearn_dataset.target)\n",
        "  # print(df)\n",
        "  # df.head()\n",
        "\n",
        "  # for 3-classes testing\n",
        "  filepath = \"/content/gdrive/My Drive/cis519/Codes/Homework/HW4/ChocolatePipes_trainData.csv\"\n",
        "  X_train_raw = pd.read_csv(filepath)      # header = None removed so the index will be applied\n",
        "  \n",
        "  filepath = \"/content/gdrive/My Drive/cis519/Codes/Homework/HW4/ChocolatePipes_trainLabels.csv\"\n",
        "  y_label_raw = pd.read_csv(filepath)\n",
        "\n",
        "  #======= preprocess the X_train_raw and y_label_raw ===============\n",
        "  # match the X and y according to the ID and remove the trash columns\n",
        "  df = X_train_raw.merge(y_label_raw, how = 'inner', on ='id')    # intersection of id, if not matched then drop\n",
        "  df = df.drop(columns = ['id', 'Date of entry', 'Recorded by', 'Country of factory', 'Country funded by'])      # drop the id column after matching, and drop Date of entry and Recorded by\n",
        "\n",
        "  # filling the NaN with features' mode contents \n",
        "  df_mode = df.mode(axis = 0).iloc[0, :]   # mode may have two rows \n",
        "  df = df.fillna(df_mode, axis=0)    # use the first mode content to replace\n",
        "\n",
        "  # apply one-hot encoding to it\n",
        "  df_getdummies = pd.get_dummies(df)\n",
        "  df = df_getdummies.drop(labels= df_getdummies.columns[df_getdummies.dtypes == object], axis=1) # is the dtype is object(str) drop the column\n",
        "\n",
        "  # deal with the size of pool \n",
        "  size_pool = df.loc[:, 'Size of chocolate pool'].copy().to_numpy()\n",
        "  size_pool_replace = np.where(size_pool == 0, np.nan, size_pool)\n",
        "  size_mode = stats.mode(size_pool_replace, nan_policy= 'omit')\n",
        "  size_mean = np.nanmean(size_pool_replace)\n",
        "  # df.loc[:,'Size of chocolate pool'].replace(0, float(size_mode[0]), inplace=True)\n",
        "  df.loc[:,'Size of chocolate pool'].replace(0, size_mean, inplace=True)\n",
        "  # print(df.loc[:,'Size of chocolate pool'].mean())\n",
        " \n",
        "  # replace oulier with modes\n",
        "  # df_mode = df.mode(axis = 0).iloc[0, :]   # mode may have two rows \n",
        "\n",
        "\n",
        "  #~~~~~~ df is the combination of preprocessed X and y\n",
        "  \n",
        "  #=============== Start the prediction (unchanged) =========================\n",
        "  # split randomly into training/testing\n",
        "  train, test = train_test_split(df, test_size=0.5, random_state=42)\n",
        "\n",
        "  # Split into X,y matrices   FOR PART ONE TESTING AND COMPARING \n",
        "  # X_train = train.drop(['CLASS'], axis=1)\n",
        "  # y_train = train['CLASS']\n",
        "  # X_test = test.drop(['CLASS'], axis=1)\n",
        "  # y_test = test['CLASS']\n",
        "\n",
        "  # FOR WONKA MODEL TRAINING\n",
        "  X_train = train.drop(['label'], axis = 1)\n",
        "  y_train = train['label']\n",
        "  X_test = test.drop(['label'], axis = 1)\n",
        "  y_test = test['label']\n",
        "\n",
        "  #============== FOR SELF TEST ======================\n",
        "  # boostingValues = [100, 125, 150, 175, 200, 225, 250]\n",
        "  # treedepthValues = [1, 2, 3, 4, 5]\n",
        "  # boostingValues = [100,  150,  200,  250, 300]\n",
        "  # treedepthValues = [10, 12, 14]\n",
        "  # score = np.zeros((len(boostingValues), len(treedepthValues)))\n",
        "  # for b, boostingiter in enumerate(boostingValues):\n",
        "  #   for t, treedepth in enumerate(treedepthValues):\n",
        "  #     modelBoostedDT = BoostedDT(numBoostingIters=boostingiter, maxTreeDepth=treedepth)    # given different values for tuning\n",
        "  #     modelBoostedDT.fit(X_train, y_train)\n",
        "  #     ypred_BoostedDT = modelBoostedDT.predict(X_test)\n",
        "  #     accuracy_BoostedDT = accuracy_score(y_test, ypred_BoostedDT)\n",
        "  #     score[b, t] = accuracy_BoostedDT\n",
        "  #     print('====================')\n",
        "  #     print('boosting iter = '+str(boostingiter)+ '  tree depth = '+str(treedepth) )\n",
        "  #     print(\"My Boosted Decision Tree Accuracy = \"+str(accuracy_BoostedDT))\n",
        "  # best_score = np.max(score)\n",
        "  # best_index = np.argwhere(score == best_score)\n",
        "  # print('Best score is:'+str(best_score)+'  index is: '+str(best_index))\n",
        "  # ========================================================================================\n",
        "\n",
        "  #======================= Comparing accuracy ============================\n",
        "  # train the decision tree\n",
        "  modelDT = DecisionTreeClassifier()\n",
        "  modelDT.fit(X_train, y_train)\n",
        "\n",
        "  # train the boosted DT\n",
        "  modelBoostedDT = BoostedDT(numBoostingIters=300, maxTreeDepth=10)\n",
        "  modelBoostedDT.fit(X_train, y_train)\n",
        "\n",
        "  # train sklearn's implementation of Adaboost\n",
        "  modelSKBoostedDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100)    # 4 & 150\n",
        "  modelSKBoostedDT.fit(X_train, y_train)\n",
        "\n",
        "  # output predictions on the test data\n",
        "  # ypred_DT = modelDT.predict(X_test)\n",
        "  # ypred_BoostedDT = modelBoostedDT.predict(X_test)\n",
        "  # ypred_SKBoostedDT = modelSKBoostedDT.predict(X_test)\n",
        "\n",
        "  ypred_DT = modelDT.predict(X_train)\n",
        "  ypred_BoostedDT = modelBoostedDT.predict(X_train)\n",
        "  ypred_SKBoostedDT = modelSKBoostedDT.predict(X_train)\n",
        "\n",
        "\n",
        "  # compute the training accuracy of the model\n",
        "  # accuracy_DT = accuracy_score(y_test, ypred_DT)\n",
        "  # accuracy_BoostedDT = accuracy_score(y_test, ypred_BoostedDT)\n",
        "  # accuracy_SKBoostedDT = accuracy_score(y_test, ypred_SKBoostedDT)\n",
        "\n",
        "  accuracy_DT = accuracy_score(y_train, ypred_DT)\n",
        "  accuracy_BoostedDT = accuracy_score(y_train, ypred_BoostedDT)\n",
        "  accuracy_SKBoostedDT = accuracy_score(y_train, ypred_SKBoostedDT)\n",
        "\n",
        "\n",
        "  print(\"Decision Tree Accuracy = \"+str(accuracy_DT))\n",
        "  print(\"My Boosted Decision Tree Accuracy = \"+str(accuracy_BoostedDT))\n",
        "  print(\"Sklearn's Boosted Decision Tree Accuracy = \"+str(accuracy_SKBoostedDT))\n",
        "\n",
        "  #============================ SVC model accuracy estimation =====================\n",
        "  \n",
        "  standardizer = StandardScaler()\n",
        "  X_train_standardized = pd.DataFrame(standardizer.fit_transform(X_train))  # compute mean and stdev on training set for standardization\n",
        "  modelSVC = SVC(gamma='scale', decision_function_shape='ovo', kernel='rbf')    # random state????\n",
        "  modelSVC.fit(X_train_standardized, y_train)\n",
        "  X_test_standardized = pd.DataFrame(standardizer.transform(X_test))\n",
        "\n",
        "  # ypred_SVC = modelSVC.predict(X_test_standardized)\n",
        "  # accuracy_SVC = accuracy_score(y_test, ypred_SVC)\n",
        "\n",
        "  ypred_SVC = modelSVC.predict(X_train_standardized)\n",
        "  accuracy_SVC = accuracy_score(y_train, ypred_SVC)\n",
        "\n",
        "  print(\"Sklearn's SVC Accuracy = \"+str(accuracy_SVC))\n",
        "\n",
        "  print()\n",
        "  print(\"Note that due to randomization, your boostedDT might not always have the \")\n",
        "  print(\"exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \")\n",
        "  print(\"should be roughly equivalent and should usually exceed the standard DT.\")\n",
        "  #================================================================================================\n",
        "\n",
        "  #========================================================================================\n",
        "  #======================== Predict the unlabeled data ====================================\n",
        "  # X_modeltrain = df.drop(['label'], axis = 1)   # use the previous 'df': X and labels for training\n",
        "  # y_modeltrain = df['label']\n",
        "\n",
        "  # # =========================================================================\n",
        "  # # train models adaboost\n",
        "  # standardizer = StandardScaler()\n",
        "  # X_modeltrain_standardized = pd.DataFrame(standardizer.fit_transform(X_modeltrain))  # compute mean and stdev on training set for standardization\n",
        "\n",
        "  # modelBoostedDT = BoostedDT(numBoostingIters=300, maxTreeDepth=10)\n",
        "  # modelBoostedDT.fit(X_modeltrain_standardized, y_modeltrain)\n",
        "  # # train SVC model\n",
        "  # # standardizer = StandardScaler()\n",
        "  # # X_modeltrain_standardized = pd.DataFrame(standardizer.fit_transform(X_modeltrain))  # compute mean and stdev on training set for standardization\n",
        "  # modelSVC = SVC(gamma='scale', decision_function_shape='ovo', kernel='rbf')    # 'standardizer' has been stored the std() and mean, use 'transform' next time\n",
        "  # modelSVC.fit(X_modeltrain_standardized, y_modeltrain)\n",
        "\n",
        "  # # train simple DT model\n",
        "  # modelDT = DecisionTreeClassifier()\n",
        "  # modelDT.fit(X_modeltrain_standardized, y_modeltrain)\n",
        "  # #========================================================================\n",
        "\n",
        "  # # read the unlabeled csv\n",
        "  # filepath_grading = \"/content/gdrive/My Drive/cis519/Codes/Homework/HW4/ChocolatePipes_gradingTestData.csv\"\n",
        "  # X_unlabeled_grading = pd.read_csv(filepath_grading)\n",
        "\n",
        "  # filepath_leader = \"/content/gdrive/My Drive/cis519/Codes/Homework/HW4/ChocolatePipes_leaderboardTestData.csv\"\n",
        "  # X_unlabeled_leader = pd.read_csv(filepath_leader)\n",
        "  \n",
        "  # # preprocess the unlabeled data\n",
        "  # ####### 1. grading dataset ///// X_unlabeled_grading\n",
        "  # id_grading = X_unlabeled_grading.loc[:, ['id']]   # extract the id \n",
        "  # X_unlabeled_grading = X_unlabeled_grading.drop(columns = ['id', 'Date of entry', 'Recorded by', 'Country of factory', 'Country funded by']) \n",
        "\n",
        "  # grading_mode = X_unlabeled_grading.mode(axis = 0).iloc[0, :]   # mode may have two rows \n",
        "  # X_unlabeled_grading = X_unlabeled_grading.fillna(grading_mode, axis=0)    # use the first mode content to replace\n",
        "\n",
        "  # grading_getdummies = pd.get_dummies(X_unlabeled_grading)\n",
        "  # X_unlabeled_grading = grading_getdummies.drop(labels= grading_getdummies.columns[grading_getdummies.dtypes == object], axis=1)  \n",
        "\n",
        "  # # adaboost\n",
        "  # X_unlabeled_grading_standardized = pd.DataFrame(standardizer.transform(X_unlabeled_grading))  # standardize the X 'standardizer.transform()'\n",
        "  # ypred_grading_BoostedDT = modelBoostedDT.predict(X_unlabeled_grading_standardized)\n",
        "  # ypred_grading = pd.DataFrame(ypred_grading_BoostedDT, columns=['label'])\n",
        "  # grading_output = pd.concat([id_grading, ypred_grading], axis=1)    # concat the id and predicted label of grading data\n",
        "  # # print(grading_output.describe())     #######\n",
        "  # grading_output.to_csv('/content/gdrive/My Drive/cis519/Codes/Homework/HW4/predictions-grading-BoostedDT.csv', index = 0)\n",
        "  \n",
        "  # #SVC\n",
        "  # X_unlabeled_grading_standardized = pd.DataFrame(standardizer.transform(X_unlabeled_grading))  # standardize the X 'standardizer.transform()'\n",
        "  # ypred_grading_SVC = modelSVC.predict(X_unlabeled_grading_standardized)\n",
        "  # ypred_grading_SVC = pd.DataFrame(ypred_grading_SVC, columns=['label'])\n",
        "  # grading_output = pd.concat([id_grading, ypred_grading_SVC], axis=1)    # concat the id and predicted label of grading data from SVC\n",
        "  # # print(grading_output.describe())     #######\n",
        "  # grading_output.to_csv('/content/gdrive/My Drive/cis519/Codes/Homework/HW4/predictions-grading-SVC.csv', index = 0)\n",
        "  \n",
        "\n",
        "\n",
        "  # ####### 2. leaderboard dataset ////// X_unlabeled_leader\n",
        "  # id_leader = X_unlabeled_leader.loc[:, ['id']]   # extract the id \n",
        "  # X_unlabeled_leader = X_unlabeled_leader.drop(columns = ['id', 'Date of entry', 'Recorded by', 'Country of factory', 'Country funded by']) \n",
        "\n",
        "  # leader_mode = X_unlabeled_leader.mode(axis = 0).iloc[0, :]   # mode may have two rows \n",
        "  # X_unlabeled_leader = X_unlabeled_leader.fillna(leader_mode, axis=0)    # use the first mode content to replace\n",
        "\n",
        "  # leader_getdummies = pd.get_dummies(X_unlabeled_leader)\n",
        "  # X_unlabeled_leader = leader_getdummies.drop(labels= leader_getdummies.columns[leader_getdummies.dtypes == object], axis=1)  \n",
        "\n",
        "  # # adaboost \n",
        "  # X_unlabeled_leader_standardized = pd.DataFrame(standardizer.transform(X_unlabeled_leader))  # standardize the X\n",
        "  # ypred_leader_BoostedDT = modelBoostedDT.predict(X_unlabeled_leader_standardized)\n",
        "  # ypred_leader = pd.DataFrame(ypred_leader_BoostedDT, columns=['label'])\n",
        "  # leader_output = pd.concat([id_leader, ypred_leader], axis=1)   \n",
        "  # leader_output.to_csv('/content/gdrive/My Drive/cis519/Codes/Homework/HW4/predictions-leaderboard-BoostedDT.csv', index = 0)\n",
        "\n",
        "  # # SVC\n",
        "  # X_unlabeled_leader_standardized = pd.DataFrame(standardizer.transform(X_unlabeled_leader))  # standardize the X\n",
        "  # ypred_leader_SVC = modelSVC.predict(X_unlabeled_leader_standardized)\n",
        "  # ypred_leader_SVC = pd.DataFrame(ypred_leader_SVC, columns=['label'])\n",
        "  # leader_output = pd.concat([id_leader, ypred_leader_SVC], axis=1)    \n",
        "  # leader_output.to_csv('/content/gdrive/My Drive/cis519/Codes/Homework/HW4/predictions-leaderboard-SVC.csv', index = 0)\n",
        "\n",
        "  # # simple DT\n",
        "  # X_unlabeled_leader_standardized = pd.DataFrame(standardizer.transform(X_unlabeled_leader))  # standardize the X\n",
        "  # ypred_leader_modelDT = modelDT.predict(X_unlabeled_leader_standardized)\n",
        "  # ypred_leader_modelDT = pd.DataFrame(ypred_leader_modelDT, columns=['label'])\n",
        "  # leader_output = pd.concat([id_leader, ypred_leader_modelDT], axis=1)    \n",
        "  # leader_output.to_csv('/content/gdrive/My Drive/cis519/Codes/Homework/HW4/predictions-leaderboard-modelDT.csv', index = 0)\n",
        "\n",
        "test_boostedDT()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Decision Tree Accuracy = 0.9962920827539713\n",
            "My Boosted Decision Tree Accuracy = 0.9893818733409178\n",
            "Sklearn's Boosted Decision Tree Accuracy = 0.7491678253908061\n",
            "Sklearn's SVC Accuracy = 0.7385918341549741\n",
            "\n",
            "Note that due to randomization, your boostedDT might not always have the \n",
            "exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \n",
            "should be roughly equivalent and should usually exceed the standard DT.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMazl_eoKZwO",
        "colab_type": "text"
      },
      "source": [
        "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
        "Decision Tree Accuracy = 0.7333361422431954\n",
        "My Boosted Decision Tree Accuracy = 0.7801044914468694\n",
        "Sklearn's Boosted Decision Tree Accuracy = 0.732914805763883\n",
        "Sklearn's SVC Accuracy = 0.713238392179995\n",
        "\n",
        "Note that due to randomization, your boostedDT might not always have the \n",
        "exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \n",
        "should be roughly equivalent and should usually exceed the standard DT."
      ]
    }
  ]
}